<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Towards Accessibility Assessment Tools for the Internet of Things"><title>Skill-Learning | Accessible IoT</title><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Inconsolata:wght@400;700&display=swap" rel=stylesheet><link rel=stylesheet href=/css/main.css></head><body><div class=site-wrapper><aside class=sidebar><div class=sidebar-header><a href=/ class=site-title>Accessible IoT</a></div><nav class=sidebar-nav><ul><li><a href=/>Home</a></li><li><a href=/research/>Research</a></li><li><a href=/timeline/>Timeline</a></li><li><a href=/news/>News</a></li><li><a href=/background/>Background</a></li><li><a href=/team/>Team</a></li></ul></nav></aside><header class=mobile-header><a href=/ class=mobile-title>Accessible IoT</a><nav class=mobile-nav><ul><li><a href=/>Home</a></li><li><a href=/research/>Research</a></li><li><a href=/timeline/>Timeline</a></li><li><a href=/news/>News</a></li><li><a href=/background/>Background</a></li><li><a href=/team/>Team</a></li></ul></nav></header><main class=main-content><div class=container><h1>Skill-Learning</h1><article class=card><h3><a href=https://accessible-iot.org/publications/zhang2025pro/>Pro's Eyes: A Wearable System for Synchronous and Asynchronous Observational Pattern Learning</a></h3><p>Pro&rsquo;s Eyes is a wearable system that enables observational pattern learning from experts in both synchronous and asynchronous modes. The system captures and analyzes expert behavior patterns, facilitating skill transfer and learning through smart glasses technology.</p></article><article class=card><h3><a href=https://accessible-iot.org/publications/chen2025multimodal/>A Multimodal Wearable Sensing System for Vocal Muscle Biofeedback in Singing Pitch Training</a></h3><p>This paper presents a multimodal wearable sensing system designed to provide vocal muscle biofeedback during singing pitch training. The system helps users develop better control over their vocal muscles through real-time physiological feedback.</p></article><article class=card><h3><a href=https://accessible-iot.org/publications/chen2025exploring/>Exploring Singing Breath: Physiological Insights and Directions for Breath-Aware Augmentation in Mixed Reality Design</a></h3><p>This paper explores the physiological aspects of singing breath control and proposes design directions for breath-aware augmentation in mixed reality. The work provides insights into respiratory patterns during singing and how they can be leveraged for immersive training experiences.</p></article><article class=card><h3><a href=https://accessible-iot.org/publications/chen2025introduction/>Introduction of the VCSD Dataset: A Vocal Cords Dataset Using EMG and Ultrasonography for Singing Pitch Skill Recognition</a></h3><p>This paper introduces VCSD (Vocal Cords for Singing Dataset), a novel dataset combining EMG and ultrasonography data for singing pitch skill recognition. The dataset provides a valuable resource for researchers working on vocal training systems and physiological sensing.</p></article><article class=card><h3><a href=https://accessible-iot.org/publications/chen2024novel/>A Novel Sensing Method and Its Empirical Study for Vocal Technique Analysis of Singing Pitch Control: Combining Surface EMG with Ultrasonography</a></h3><p>This paper introduces a novel sensing method that combines surface EMG with ultrasonography to analyze vocal techniques during singing pitch control. The multimodal approach provides deeper insights into the physiological mechanisms of singing and opens new possibilities for vocal training systems.</p></article></div></main></div></body></html>